{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdef969",
   "metadata": {},
   "source": [
    "## CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e45e61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d22fa131",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json_data = [] # a list of json records\n",
    "\n",
    "with open(\"documents.csv\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        json_obj = {\n",
    "            \"doc_id\": row[\"ID\"], \n",
    "            \"text\": row[\"Text\"] \n",
    "        }\n",
    "        all_json_data.append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae1d54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18316\n"
     ]
    }
   ],
   "source": [
    "print(len(all_json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d12df",
   "metadata": {},
   "source": [
    "## Start Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8377895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d824ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44264ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the Vector Space Model for our IR-method\n",
    "vsm_mapping = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"similarity\": {\n",
    "            \"scripted_tfidf\": {\n",
    "                \"type\": \"scripted\",\n",
    "                \"script\": {\n",
    "                    \"source\": \"double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\"\n",
    "                }\n",
    "            }\n",
    "        },        \n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"default\": {\n",
    "                    \"type\": \"english\"\n",
    "                },\n",
    "                \"default_search\": {\n",
    "                    \"type\": \"english\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"doc_id\":{\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"scripted_tfidf\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e4221f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete any existing index\n",
    "client.indices.delete(index=\"my_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e6a11dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'my_docs'})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the new index\n",
    "client.indices.create(index='my_docs', body=vsm_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc957a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing documents into `my_docs` index!\n"
     ]
    }
   ],
   "source": [
    "def insert_document(document):\n",
    "    \"\"\"Index one document.\n",
    "\n",
    "    We force the Elasticsearch _id to be equal to our collection's doc_id (from CSV).\n",
    "    This way, hit[\"_id\"] == hit[\"_source\"][\"doc_id\"] and we avoid mismatches.\n",
    "    \"\"\"\n",
    "    docid = str(document[\"doc_id\"]) # Get the doc id of each document\n",
    "    return client.index(index=\"my_docs\", id=docid, body=document) # Set the dock id as the index id\n",
    "\n",
    "# populate index one document at a time\n",
    "for document in all_json_data:\n",
    "    insert_document(document)\n",
    "\n",
    "print(\"Done indexing documents into `my_docs` index!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63ed57b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 18316, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "# The count must be 18,316\n",
    "count = client.count(index=\"my_docs\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1504256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_response(response, max_hits=10):\n",
    "    \"\"\"DEBUG FUNCTION\n",
    "    Help us check if the response we got has the format that we expect\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use doc_id from _source (collection ID), NOT the ES internal _id.\n",
    "    hits = response.get(\"hits\", {}).get(\"hits\", []) # Get the rersults of the search\n",
    "    if not hits: \n",
    "        print(\"Your search returned no results.\")\n",
    "        return\n",
    "\n",
    "    for i, hit in enumerate(hits[:max_hits], start=1):\n",
    "        es_id = hit.get(\"_id\") # Get elastic search id (index id)\n",
    "        score = hit.get(\"_score\") # Get the tf-idf score \n",
    "        src = hit.get(\"_source\", {}) \n",
    "        doc_id = src.get(\"doc_id\")  # Get the doc id\n",
    "        text_preview = (src.get(\"text\",\"\")[:120] + \"…\") if src.get(\"text\") else \"\"\n",
    "        print(f\"{i:>2}. doc_id={doc_id} | _id={es_id} | score={score:.4f} | {text_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20279d7",
   "metadata": {},
   "source": [
    "## Querying + Run file (TREC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4db7d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "QUERIES_PATH = \"queries.csv\" \n",
    "\n",
    "\n",
    "queries_df = pd.read_csv(QUERIES_PATH)  # columns: ID, Text\n",
    "\n",
    "\n",
    "print(\"queries:\", queries_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b6d0de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_search(query_text: str, k: int = 50):\n",
    "    \"\"\"Vector Space (classic) retrieval using our scripted_tfidf similarity on field 'text'.\"\"\"\n",
    "    body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"text\": {\n",
    "                    \"query\": query_text\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return client.search(index=\"my_docs\", body=body)\n",
    "\n",
    "def make_run_rows(qid: str, query_text: str, k: int, run_name: str = \"VSM\"):\n",
    "    \"\"\"Return rows in TREC run format: qid Q0 docid rank score IR-method.\"\"\"\n",
    "    resp = es_search(query_text, k=k)\n",
    "    print_search_response(resp,1)\n",
    "    rows = []\n",
    "    for rank, hit in enumerate(resp[\"hits\"][\"hits\"], start=1):\n",
    "        # use collection doc_id, not ES internal id\n",
    "        doc_id = str(hit.get(\"_source\", {}).get(\"doc_id\")) # doc id must be string in the TREC\n",
    "        score = float(hit.get(\"_score\", 0.0)) # Score must be float in the TREC\n",
    "        rows.append((qid, \"Q0\", doc_id, rank, score, run_name))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd237146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. doc_id=193378 | _id=193378 | score=88.7109 | Optimodal European Travel Ecosystem: EuTravel aims to: 1. Support the EU agenda towards an open and single market for mo…\n",
      " 1. doc_id=213164 | _id=213164 | score=61.5033 | Big Data for Mobility Tracking Knowledge Extraction in Urban Areas: Track&amp;Know will research, develop and exploit a …\n",
      " 1. doc_id=204146 | _id=204146 | score=95.3150 | Towards a Shared European Logistics Intelligent Information Space: SELIS is aimed at delivering a ‘platform for pan-Euro…\n",
      " 1. doc_id=214253 | _id=214253 | score=77.8100 | Polyglot and Hybrid Persistence Architectures for Big Data Analytics: The need for levels of availability and scalabilit…\n",
      " 1. doc_id=212490 | _id=212490 | score=89.1696 | Cognitive Heterogeneous Architecture for Industrial IoT: CHARIOT will provide a design method and cognitive computing pl…\n",
      " 1. doc_id=210133 | _id=210133 | score=78.1138 | End-to-End Approach for Mobility-as-a-Service tools, business models, enabling framework and evidence for European seaml…\n",
      " 1. doc_id=213097 | _id=213097 | score=80.3447 | SOFIE - Secure Open Federation for Internet Everywhere: SOFIE addresses the challenges of the call by creating a secure …\n",
      " 1. doc_id=193715 | _id=193715 | score=72.1672 | Data driven approach for a Seamless Efficient European Travelling in 2050: DATASET2050 (DATA driven approach for a Seaml…\n",
      " 1. doc_id=197346 | _id=197346 | score=83.2366 | 5G Exchange: The goal of the 5G Exchange (5GEx) project is to enable cross-domain orchestration of services over multipl…\n",
      " 1. doc_id=199879 | _id=199879 | score=66.5598 | network infrastructure as commons: Communication and information distribution are key components of a modern society. Th…\n",
      " 1. doc_id=193378 | _id=193378 | score=88.7109 | Optimodal European Travel Ecosystem: EuTravel aims to: 1. Support the EU agenda towards an open and single market for mo…\n",
      " 1. doc_id=213164 | _id=213164 | score=61.5033 | Big Data for Mobility Tracking Knowledge Extraction in Urban Areas: Track&amp;Know will research, develop and exploit a …\n",
      " 1. doc_id=204146 | _id=204146 | score=95.3150 | Towards a Shared European Logistics Intelligent Information Space: SELIS is aimed at delivering a ‘platform for pan-Euro…\n",
      " 1. doc_id=214253 | _id=214253 | score=77.8100 | Polyglot and Hybrid Persistence Architectures for Big Data Analytics: The need for levels of availability and scalabilit…\n",
      " 1. doc_id=212490 | _id=212490 | score=89.1696 | Cognitive Heterogeneous Architecture for Industrial IoT: CHARIOT will provide a design method and cognitive computing pl…\n",
      " 1. doc_id=210133 | _id=210133 | score=78.1138 | End-to-End Approach for Mobility-as-a-Service tools, business models, enabling framework and evidence for European seaml…\n",
      " 1. doc_id=213097 | _id=213097 | score=80.3447 | SOFIE - Secure Open Federation for Internet Everywhere: SOFIE addresses the challenges of the call by creating a secure …\n",
      " 1. doc_id=193715 | _id=193715 | score=72.1672 | Data driven approach for a Seamless Efficient European Travelling in 2050: DATASET2050 (DATA driven approach for a Seaml…\n",
      " 1. doc_id=197346 | _id=197346 | score=83.2366 | 5G Exchange: The goal of the 5G Exchange (5GEx) project is to enable cross-domain orchestration of services over multipl…\n",
      " 1. doc_id=199879 | _id=199879 | score=66.5598 | network infrastructure as commons: Communication and information distribution are key components of a modern society. Th…\n",
      " 1. doc_id=193378 | _id=193378 | score=88.7109 | Optimodal European Travel Ecosystem: EuTravel aims to: 1. Support the EU agenda towards an open and single market for mo…\n",
      " 1. doc_id=213164 | _id=213164 | score=61.5033 | Big Data for Mobility Tracking Knowledge Extraction in Urban Areas: Track&amp;Know will research, develop and exploit a …\n",
      " 1. doc_id=204146 | _id=204146 | score=95.3150 | Towards a Shared European Logistics Intelligent Information Space: SELIS is aimed at delivering a ‘platform for pan-Euro…\n",
      " 1. doc_id=214253 | _id=214253 | score=77.8100 | Polyglot and Hybrid Persistence Architectures for Big Data Analytics: The need for levels of availability and scalabilit…\n",
      " 1. doc_id=212490 | _id=212490 | score=89.1696 | Cognitive Heterogeneous Architecture for Industrial IoT: CHARIOT will provide a design method and cognitive computing pl…\n",
      " 1. doc_id=210133 | _id=210133 | score=78.1138 | End-to-End Approach for Mobility-as-a-Service tools, business models, enabling framework and evidence for European seaml…\n",
      " 1. doc_id=213097 | _id=213097 | score=80.3447 | SOFIE - Secure Open Federation for Internet Everywhere: SOFIE addresses the challenges of the call by creating a secure …\n",
      " 1. doc_id=193715 | _id=193715 | score=72.1672 | Data driven approach for a Seamless Efficient European Travelling in 2050: DATASET2050 (DATA driven approach for a Seaml…\n",
      " 1. doc_id=197346 | _id=197346 | score=83.2366 | 5G Exchange: The goal of the 5G Exchange (5GEx) project is to enable cross-domain orchestration of services over multipl…\n",
      " 1. doc_id=199879 | _id=199879 | score=66.5598 | network infrastructure as commons: Communication and information distribution are key components of a modern society. Th…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{20: 'trec_eval\\\\vsm_k20.txt',\n",
       " 30: 'trec_eval\\\\vsm_k30.txt',\n",
       " 50: 'trec_eval\\\\vsm_k50.txt'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_trec(run_rows, out_path: str):\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for qid, q0, doc_id, rank, score, run_name in run_rows: \n",
    "            f.write(f\"{qid} {q0} {doc_id} {rank} {score:.6f} {run_name}\\n\") # TREC format\n",
    "    return str(out_path)\n",
    "\n",
    "# create run files for k = 20, 30, 50\n",
    "run_files = {}\n",
    "for k in [20, 30, 50]:\n",
    "    rows = []\n",
    "    for _, r in queries_df.iterrows():\n",
    "        qid = str(r[\"ID\"])\n",
    "        qtext = str(r[\"Text\"])\n",
    "        rows.extend(make_run_rows(qid, qtext, k=k, run_name=f\"VSM_k{k}\"))\n",
    "    run_files[k] = write_trec(rows, f\"trec_eval/vsm_k{k}.txt\")\n",
    "\n",
    "run_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
